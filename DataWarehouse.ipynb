{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Business Process Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. User melihat-lihat produk di platform\n",
    "2. User menambahkan produk ke keranjang belanja\n",
    "3. User melakukan checkout dan membuat pesanan\n",
    "4. Sistem membuat dan mencatat detail pesanan seperti product, harga dan informasi pengiriman\n",
    "5. Pesanan diproses dan disiapkan di distribution center\n",
    "6. Pesanan dikirim ke User\n",
    "7. Pesanan diterima oleh User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, we will use a single fact table that focuses on sales:\n",
    "\n",
    "Fact_Penjualan  \n",
    "* order_id (Foreign key ke Dim_Order)\n",
    "* users_id (Foreign Key ke Dim_Users)\n",
    "* products_id (Foreign Key ke Dim_Products)\n",
    "* distribution_center_id (Foreign Key ke Dim_Distribusi)\n",
    "* jumlah_item (Jumlah item yang terjual atau dikembalikan)\n",
    "* harga_satuan (Harga satuan per item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dimension Table <br>\n",
    "Dimension table akan menyimpan informasi deskriptif terkait dengan fact table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension Table: Dim_Users\n",
    "\n",
    "Dim_Users: Informasi pelanggan.\n",
    "users_id (Primary Key)\n",
    "first_name, last_name, email, age, gender, state, city, country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension Table: Dim_Products\n",
    "\n",
    "Dim_Products: Informasi produk.\n",
    "products_id (Primary Key)\n",
    "product_name, cost, category, brand, retail_price, department, sku\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension Table: Dim_Order\n",
    "\n",
    "Dim_Order: Informasi order\n",
    "order_id (Primary Key)\n",
    "status, created_at, returned_at, shipped_at, delivered_at, num_of_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension Table: Dim_OrderItems\n",
    "\n",
    "Dim_OrderItems: Informasi Item order.\n",
    "Orderitems_id (Primary Key)\n",
    "status, created_at, returned_at, shipped_at, delivered_at, sale_price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension Table: Dim_Distribusi\n",
    "\n",
    "Dim_Distribusi: Informasi distribution center.\n",
    "distribusi_id (Primary Key)\n",
    "distribusi_geo, distribusi_name, latitude, longitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penjelasan:\n",
    "* Desain data warehouse ini menggunakan skema Star Schema. Skema ini dipilih karena kesederhanaannya dan kemudahan dalam melakukan query analisis. Fact table Fact_Penjualan berada di tengah, dikelilingi oleh dimension table yang terhubung langsung.\n",
    "* Tiap  dimension table itu memberikan konteks deskriptif untuk data penjualan Misalnya, Dim_Pelanggan memberikan informasi tentang siapa yang membeli produk, Dim_Produk memberikan informasi tentang produk apa yang dibeli, dan seterusnya.\n",
    "* Hubungan antara fact table dan dimension table adalah one-to-many. Satu record di dimension table dapat dihubungkan dengan banyak record di fact table.\n",
    "\n",
    "Google Slide : https://docs.google.com/presentation/d/1VA2u2p26w_f5fUNoq7upYZj-XMQjbReAogvrbkN6zXA/edit#slide=id.g3303b384963_0_429\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://158a234c62a7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.0-preview2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://158a234c62a7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f64f5b03650>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memasukan data CSV dan Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----+----+-----------+------------+\n",
      "|order_id| id|id_1|id_2|num_of_item|retail_price|\n",
      "+--------+---+----+----+-----------+------------+\n",
      "|       7|  7|   7|   7|          1|        39.5|\n",
      "|       2|  2|   2|   2|          1|        69.5|\n",
      "|       5|  5|   5|   5|          1|        94.0|\n",
      "|       8|  8|   8|   8|          1|       168.0|\n",
      "|       9|  9|   9|   9|          1|        54.0|\n",
      "|       6|  6|   6|   6|          1|       132.0|\n",
      "|       1|  1|   1|   1|          2|        49.0|\n",
      "|       3|  3|   3|   3|          1|        69.5|\n",
      "|       4|  4|   4|   4|          1|       108.0|\n",
      "|      10| 10|  10|  10|          1|        59.5|\n",
      "+--------+---+----+----+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FactTable = spark.read.csv('csv/FactTable.csv', header=True, inferSchema=True)\n",
    "FactTable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+--------------------+---+------+-----+--------------+-------+\n",
      "|   id|first_name|last_name|               email|age|gender|state|          city|country|\n",
      "+-----+----------+---------+--------------------+---+------+-----+--------------+-------+\n",
      "| 7351|    Teresa| Robinson|teresarobinson@ex...| 55|     F| Acre|          null| Brasil|\n",
      "|58395|      Paul|  Mueller|paulmueller@examp...| 28|     M| Acre|          null| Brasil|\n",
      "|58804|     Angel|     Lane|angellane@example...| 69|     F| Acre|          null| Brasil|\n",
      "|19200|     James|     Bell|jamesbell@example...| 63|     M| Acre|          null| Brasil|\n",
      "|95648|     Julia|  Richard|juliarichard@exam...| 37|     F| Acre|          null| Brasil|\n",
      "|79611|    Victor|      Cox|victorcox@example...| 59|     M| Acre|          null| Brasil|\n",
      "|68040|      John|   Obrien|johnobrien@exampl...| 13|     M| Acre|          null| Brasil|\n",
      "| 5811|    Ashley|  Johnson|ashleyjohnson@exa...| 66|     F| Acre|          null| Brasil|\n",
      "|16079|      Eric|   Bryant|ericbryant@exampl...| 52|     M| Acre|          null| Brasil|\n",
      "|38045|     Megan|   Adkins|meganadkins@examp...| 13|     F| Acre|          null| Brasil|\n",
      "|98264|  Jennifer|    Mejia|jennifermejia@exa...| 17|     F| Acre|          null| Brasil|\n",
      "|44838|     Terry|    Barry|terrybarry@exampl...| 61|     M| Acre|          null| Brasil|\n",
      "|20208|    Edward|    Jones|edwardjones@examp...| 28|     M| Acre|          null| Brasil|\n",
      "|90867|     Jamie|  Johnson|jamiejohnson@exam...| 14|     F| Acre|          null| Brasil|\n",
      "|17986|     Tyler|   Butler|tylerbutler@examp...| 21|     M| Acre|          null| Brasil|\n",
      "|74105|       Ana|  Sweeney|anasweeney@exampl...| 69|     F| Acre|          null| Brasil|\n",
      "|47979|    Alfred|    Davis|alfreddavis@examp...| 30|     M| Acre|      Tarauacá| Brasil|\n",
      "|95403|   Caitlin| Martinez|caitlinmartinez@e...| 45|     F| Acre|      Tarauacá| Brasil|\n",
      "|60783|       Lee|   Juarez|leejuarez@example...| 60|     M| Acre|      Tarauacá| Brasil|\n",
      "| 9524|   Rebecca|  Sherman|rebeccasherman@ex...| 21|     F| Acre|Sena Madureira| Brasil|\n",
      "+-----+----------+---------+--------------------+---+------+-----+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Dim_Users = spark.read.csv('csv/Dim_Users.csv', header=True, inferSchema=True)\n",
    "Dim_Users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+-----------+-----+------------------+----------+--------------------+\n",
      "|   id|                name|              cost|   category|brand|      retail_price|department|                 sku|\n",
      "+-----+--------------------+------------------+-----------+-----+------------------+----------+--------------------+\n",
      "|13842|Low Profile Dyed ...| 2.518749990849756|Accessories|   MG|              6.25|     Women|EBD58B8A3F1D72F42...|\n",
      "|13928|Low Profile Dyed ...|2.3383499148894105|Accessories|   MG| 5.949999809265137|     Women|2EAC42424D12436BD...|\n",
      "|14115|Enzyme Regular So...| 4.879559879379869|Accessories|   MG|10.989999771118164|     Women|EE364229B2791D1EF...|\n",
      "|14157|Enzyme Regular So...| 4.648769887297898|Accessories|   MG|10.989999771118164|     Women|00BD13095D06C20B1...|\n",
      "|14273|Washed Canvas Ivy...| 6.507929886473045|Accessories|   MG|15.989999771118164|     Women|F531DC20FDE20B7AD...|\n",
      "|15674|Low Profile Dyed ...|3.1062499998370185|       Plus|   MG|              6.25|     Women|63894CE404B8C6529...|\n",
      "|15816|Low Profile Dyed ...|3.1772999091416594|       Plus|   MG| 5.949999809265137|     Women|151EA8C2D98CE89C2...|\n",
      "|28646|4 Panel Large Bil...|  8.73562987972319|Accessories|   MG|19.989999771118164|       Men|789334DE6DAA80D83...|\n",
      "|28670|Low Profile Dyed ...|2.6759399153566363|Accessories|   MG| 6.179999828338623|       Men|E74843B99DA8B2977...|\n",
      "|28714|Low Profile Dyed ...| 2.275000000372529|Accessories|   MG|              6.25|       Men|8CA33D44648CC9FEE...|\n",
      "|28779|Fishing Hat (01)-...|  7.36748991528362|Accessories|   MG|20.989999771118164|       Men|167D739013D283895...|\n",
      "|28904|Fashion Plaid Ivy...|6.7957498848550015|Accessories|   MG|15.989999771118164|       Men|0EBD0B8B51EB0D006...|\n",
      "|29007|Washed Hunting Fi...|10.795999868229032|Accessories|   MG|26.989999771118164|       Men|11172787BDF65BA27...|\n",
      "|12777|Womens MW Tankini...|16.691290760814255|       Swim|   MW|  44.9900016784668|     Women|553C3741E8D893FE9...|\n",
      "|12810|Mw Long Ruffle Ta...|21.055320884742144|       Swim|   MW|  44.9900016784668|     Women|5A1106FCB6C233176...|\n",
      "|12812|Womens long tanki...| 21.14530090284795|       Swim|   MW|  44.9900016784668|     Women|179DB74797BDE5F37...|\n",
      "|12853|Womens MW Tankini...| 18.71584077014294|       Swim|   MW|  44.9900016784668|     Women|5EF893A3104AB0CC8...|\n",
      "|12900|Womens One Piece/...| 17.39565078823306|       Swim|   MW|  39.9900016784668|     Women|09EA221D3DB11DF1F...|\n",
      "|13247|Womens swimsuit M...| 20.65041088706643|       Swim|   MW|  44.9900016784668|     Women|EB61C2B1B1BF5D25C...|\n",
      "|13477|MW Women's Swimsu...|22.600890809484422|       Swim|   MW|  54.9900016784668|     Women|DBC50898CF582DFEB...|\n",
      "+-----+--------------------+------------------+-----------+-----+------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Dim_Products = spark.read.csv('csv/Dim_Products.csv', header=True, inferSchema=True)\n",
    "Dim_Products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------------------+-----------+----------+------------+-----------+\n",
      "|order_id|   status|          created_at|returned_at|shipped_at|delivered_at|num_of_item|\n",
      "+--------+---------+--------------------+-----------+----------+------------+-----------+\n",
      "|       7|Cancelled| 2021-03-13 07:52:00|       NULL|      NULL|        NULL|          1|\n",
      "|      14|Cancelled| 2021-09-12 14:02:00|       NULL|      NULL|        NULL|          3|\n",
      "|      24|Cancelled| 2022-12-07 15:16:00|       NULL|      NULL|        NULL|          1|\n",
      "|      26|Cancelled| 2023-10-03 04:22:00|       NULL|      NULL|        NULL|          1|\n",
      "|      79|Cancelled| 2025-01-28 14:21:00|       NULL|      NULL|        NULL|          1|\n",
      "|      90|Cancelled| 2024-07-01 10:37:00|       NULL|      NULL|        NULL|          1|\n",
      "|     140|Cancelled| 2023-02-19 16:58:00|       NULL|      NULL|        NULL|          2|\n",
      "|     144|Cancelled| 2024-04-04 18:47:00|       NULL|      NULL|        NULL|          2|\n",
      "|     153|Cancelled| 2024-06-18 07:11:00|       NULL|      NULL|        NULL|          1|\n",
      "|     161|Cancelled| 2023-01-18 09:24:00|       NULL|      NULL|        NULL|          1|\n",
      "|     168|Cancelled|2025-02-04 07:43:...|       NULL|      NULL|        NULL|          1|\n",
      "|     179|Cancelled| 2024-12-09 08:53:00|       NULL|      NULL|        NULL|          1|\n",
      "|     226|Cancelled| 2022-08-29 15:34:00|       NULL|      NULL|        NULL|          4|\n",
      "|     233|Cancelled| 2019-11-22 17:07:00|       NULL|      NULL|        NULL|          1|\n",
      "|     253|Cancelled| 2024-09-29 16:20:00|       NULL|      NULL|        NULL|          2|\n",
      "|     274|Cancelled| 2024-11-16 16:44:00|       NULL|      NULL|        NULL|          1|\n",
      "|     283|Cancelled| 2022-08-19 09:47:00|       NULL|      NULL|        NULL|          3|\n",
      "|     284|Cancelled| 2022-04-05 09:47:00|       NULL|      NULL|        NULL|          1|\n",
      "|     292|Cancelled| 2025-01-15 06:28:00|       NULL|      NULL|        NULL|          4|\n",
      "|     320|Cancelled| 2024-04-19 15:17:00|       NULL|      NULL|        NULL|          2|\n",
      "+--------+---------+--------------------+-----------+----------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Dim_Orders = spark.read.csv('csv/Dim_Orders.csv', header=True, inferSchema=True)\n",
    "Dim_Orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------------------+-------------------+-------------------+-------------------+\n",
      "|    id|   status|         created_at|        returned_at|         shipped_at|       delivered_at|\n",
      "+------+---------+-------------------+-------------------+-------------------+-------------------+\n",
      "| 73167| Complete|2023-04-20 09:54:08|               NULL|2023-04-18 13:05:00|2023-04-21 07:48:00|\n",
      "|139830| Complete|2021-01-14 22:31:39|               NULL|2021-01-14 15:42:00|2021-01-19 13:16:00|\n",
      "|156347| Complete|2024-08-01 10:48:09|               NULL|2024-08-03 04:33:00|2024-08-07 10:01:00|\n",
      "| 88338|  Shipped|2024-08-24 03:57:46|               NULL|2024-08-25 04:10:00|               NULL|\n",
      "|148166|Cancelled|2024-02-23 06:42:03|               NULL|               NULL|               NULL|\n",
      "| 76351| Complete|2022-06-09 06:38:34|               NULL|2022-06-09 15:30:00|2022-06-14 06:28:00|\n",
      "|125401| Complete|2021-11-05 13:58:23|               NULL|2021-11-07 09:43:00|2021-11-12 01:40:00|\n",
      "|132246| Complete|2022-08-15 03:30:34|               NULL|2022-08-16 07:24:00|2022-08-18 20:13:00|\n",
      "|141609| Complete|2022-11-24 09:59:09|               NULL|2022-11-24 18:46:00|2022-11-29 06:35:00|\n",
      "| 70828| Returned|2021-05-27 18:29:12|2021-05-31 07:05:00|2021-05-28 16:45:00|2021-05-29 11:10:00|\n",
      "|103007|  Shipped|2023-05-11 10:46:57|               NULL|2023-05-12 03:45:00|               NULL|\n",
      "|168792|Cancelled|2025-01-11 11:56:38|               NULL|               NULL|               NULL|\n",
      "| 24582| Complete|2023-11-15 08:31:10|               NULL|2023-11-15 19:21:00|2023-11-16 16:37:00|\n",
      "| 25831| Complete|2024-04-18 23:08:34|               NULL|2024-04-17 11:13:00|2024-04-18 07:48:00|\n",
      "| 38090| Complete|2023-09-02 00:18:42|               NULL|2023-09-02 02:56:00|2023-09-02 06:06:00|\n",
      "| 42107| Complete|2021-10-25 14:54:18|               NULL|2021-10-26 10:38:00|2021-10-28 21:03:00|\n",
      "| 31219|  Shipped|2024-07-22 02:04:17|               NULL|2024-07-20 11:56:00|               NULL|\n",
      "|147991|  Shipped|2024-04-26 06:16:28|               NULL|2024-04-26 19:49:00|               NULL|\n",
      "|160119|  Shipped|2024-05-09 12:46:33|               NULL|2024-05-12 12:11:00|               NULL|\n",
      "|173452|  Shipped|2023-02-28 23:07:32|               NULL|2023-03-01 10:44:00|               NULL|\n",
      "+------+---------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Dim_OrderItem = spark.read.csv('csv/Dim_OrderItem.csv', header=True, inferSchema=True)\n",
    "Dim_OrderItem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+--------------------+--------+---------+\n",
      "| id|distribution_center_geom|                name|latitude|longitude|\n",
      "+---+------------------------+--------------------+--------+---------+\n",
      "|  4|    POINT(-118.25 34.05)|      Los Angeles CA|   34.05|  -118.25|\n",
      "|  6|    POINT(-73.7834 40...|Port Authority of...|  40.634| -73.7834|\n",
      "|  1|    POINT(-89.9711 35...|          Memphis TN| 35.1174| -89.9711|\n",
      "|  3|    POINT(-95.3698 29...|          Houston TX| 29.7604| -95.3698|\n",
      "|  7|    POINT(-75.1667 39...|     Philadelphia PA|   39.95| -75.1667|\n",
      "|  5|    POINT(-90.0667 29...|      New Orleans LA|   29.95| -90.0667|\n",
      "|  9|    POINT(-79.9333 32...|       Charleston SC| 32.7833| -79.9333|\n",
      "|  8|    POINT(-88.0431 30...|           Mobile AL| 30.6944| -88.0431|\n",
      "|  2|    POINT(-87.6847 41...|          Chicago IL| 41.8369| -87.6847|\n",
      "| 10|    POINT(-81.1167 32...|         Savannah GA| 32.0167| -81.1167|\n",
      "+---+------------------------+--------------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Dim_Distribution = spark.read.csv('csv/Dim_distribution.csv', header=True, inferSchema=True)\n",
    "Dim_Distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek informasi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- num_of_item: integer (nullable = true)\n",
      " |-- retail_price: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- products_id: integer (nullable = true)\n",
      " |-- products_name: string (nullable = true)\n",
      " |-- cost: double (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- retail_price: double (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- sku: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- returned_at: timestamp (nullable = true)\n",
      " |-- shipped_at: timestamp (nullable = true)\n",
      " |-- delivered_at: timestamp (nullable = true)\n",
      " |-- num_of_item: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- OrderItem_id: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- returned_at: timestamp (nullable = true)\n",
      " |-- shipped_at: timestamp (nullable = true)\n",
      " |-- delivered_at: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- distribution_center_geom: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FactTable.printSchema()\n",
    "Dim_Users.printSchema()\n",
    "Dim_Products.printSchema()\n",
    "Dim_Orders.printSchema()\n",
    "Dim_OrderItem.printSchema()\n",
    "Dim_Distribution.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengganti Nama Kolom FactTable\n",
    "FactTable = FactTable.withColumnRenamed(\"id\", \"user_id\")\n",
    "FactTable = FactTable.withColumnRenamed(\"id_1\", \"products_id\")\n",
    "FactTable = FactTable.withColumnRenamed(\"id_2\", \"distribution_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengganti Nama Kolom id\n",
    "Dim_Users = Dim_Users.withColumnRenamed(\"id\", \"user_id\")\n",
    "Dim_Products = Dim_Products.withColumnRenamed(\"id\", \"products_id\")\n",
    "Dim_OrderItem = Dim_OrderItem.withColumnRenamed(\"id\", \"OrderItem_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengganti Nama Kolom\n",
    "Dim_Products = Dim_Products.withColumnRenamed(\"name\", \"products_name\")\n",
    "Dim_Distribution = Dim_Distribution.withColumnRenamed(\"id\", \"distribution_id\")\n",
    "Dim_Distribution = Dim_Distribution.withColumnRenamed(\"name\", \"distribution_center_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'user_id',\n",
       " 'products_id',\n",
       " 'distribution_id',\n",
       " 'num_of_item',\n",
       " 'retail_price']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FactTable.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'first_name',\n",
       " 'last_name',\n",
       " 'email',\n",
       " 'age',\n",
       " 'gender',\n",
       " 'state',\n",
       " 'city',\n",
       " 'country']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dim_Users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['products_id',\n",
       " 'products_name',\n",
       " 'cost',\n",
       " 'category',\n",
       " 'brand',\n",
       " 'retail_price',\n",
       " 'department',\n",
       " 'sku']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dim_Products.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'status',\n",
       " 'created_at',\n",
       " 'returned_at',\n",
       " 'shipped_at',\n",
       " 'delivered_at',\n",
       " 'num_of_item']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dim_Orders.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OrderItem_id',\n",
       " 'status',\n",
       " 'created_at',\n",
       " 'returned_at',\n",
       " 'shipped_at',\n",
       " 'delivered_at']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dim_OrderItem.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distribution_id',\n",
       " 'distribution_center_geom',\n",
       " 'distribution_center_name',\n",
       " 'latitude',\n",
       " 'longitude']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dim_Distribution.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WriteToPostgres\") \\\n",
    "    .config(\"Spark.jars.packakges\",\"org.postgresql:42.6.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAR Files:\n",
      "List()\n"
     ]
    }
   ],
   "source": [
    "# Access SparkCOntext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# List the loaded JAR files\n",
    "print(\"JAR Files:\")\n",
    "print(sc._jsc.sc().listJars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o175.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:265)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:269)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:53)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:77)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:118)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:742)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:608)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:100)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:157)\n\tat org.apache.spark.sql.internal.DataFrameWriterImpl.runCommand(DataFrameWriterImpl.scala:614)\n\tat org.apache.spark.sql.internal.DataFrameWriterImpl.saveToV1Source(DataFrameWriterImpl.scala:271)\n\tat org.apache.spark.sql.internal.DataFrameWriterImpl.saveInternal(DataFrameWriterImpl.scala:239)\n\tat org.apache.spark.sql.internal.DataFrameWriterImpl.save(DataFrameWriterImpl.scala:125)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m postgres_properties \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntaka\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Write DF to PosgreSQL\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mFactTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpostgres_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFactTable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpostgres_properties\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:2346\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   2345\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 2346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:247\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    249\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o175.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:265)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:269)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:53)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:77)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:118)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:742)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:608)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:100)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:157)\n\tat org.apache.spark.sql.internal.DataFrameWriterImpl.runCommand(DataFrameWriterImpl.scala:614)\n\tat org.apache.spark.sql.internal.DataFrameWriterImpl.saveToV1Source(DataFrameWriterImpl.scala:271)\n\tat org.apache.spark.sql.internal.DataFrameWriterImpl.saveInternal(DataFrameWriterImpl.scala:239)\n\tat org.apache.spark.sql.internal.DataFrameWriterImpl.save(DataFrameWriterImpl.scala:125)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# PostgreSQL\n",
    "postgres_url = \"jdbc:postgresql://host.docker.internal:5432/datawarehouse_thelook\"\n",
    "postgres_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"Intaka\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Write DF to PosgreSQL\n",
    "FactTable.write.jdbc(url=postgres_url, table=\"FactTable\", mode=\"overwrite\", properties=postgres_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
